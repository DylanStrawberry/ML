{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKSTLF2BX6jH"
   },
   "source": [
    "# 441B Haowen Yan 506305318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "N11Ee3GJmywu",
    "outputId": "64e10b54-dad3-48e2-c266-617670a17e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/yanhaowen/anaconda3/lib/python3.11/site-packages/visions-0.5.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/26/a1/75474477af2a1dae3a25f80b72bbaf20e8296191ece7fff2f67984206f33/openai-1.12.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/41/7b/ddacf6dcebb42466abd03f368782142baa82e08fc0c1f8eaa05b4bae87d5/httpx-0.27.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from openai) (1.10.13)\n",
      "Requirement already satisfied: sniffio in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/2c/93/13f25f2f78646bab97aee7680821e30bd85b2ff0fc45d5fdf5393b79716d/httpcore-1.0.4-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for h11<0.15,>=0.13 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/yanhaowen/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.4)\n",
      "Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m664.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=93589ee9dc4faa27602740be799cbaa630d60ac522daf29aa55b48135c8bc5d0\n",
      "  Stored in directory: /Users/yanhaowen/Library/Caches/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "\u001b[33mWARNING: Skipping /Users/yanhaowen/anaconda3/lib/python3.11/site-packages/visions-0.5.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: h11, distro, wikipedia, httpcore, httpx, openai\n",
      "\u001b[33mWARNING: Skipping /Users/yanhaowen/anaconda3/lib/python3.11/site-packages/visions-0.5.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/yanhaowen/anaconda3/lib/python3.11/site-packages/visions-0.5.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/yanhaowen/anaconda3/lib/python3.11/site-packages/visions-0.5.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0 wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q2A8TGhKm3i5"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E9HEMJSX-3T"
   },
   "source": [
    "# 1.) Set up OpenAI client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"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\"\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key = openai.api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOXc5_BTm9HP"
   },
   "source": [
    "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_titles = ['Artificial Intelligence','Machine Learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_title = page_titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "-v7OYamHlrEB"
   },
   "outputs": [],
   "source": [
    "search_results = wikipedia.search(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "TgY2FkTdmhTH"
   },
   "outputs": [],
   "source": [
    "page = wikipedia.page(search_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "ZF3BiZyXltYO"
   },
   "outputs": [],
   "source": [
    "def get_wikipedia_content(page_title):\n",
    "    search_results = wikipedia.search(page_title)\n",
    "    page = wikipedia.page(search_results[0])\n",
    "    return(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9aruncMmubX"
   },
   "source": [
    "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_content = get_wikipedia_content(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_error_correction(wiki_content):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Correct model ID\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"I will be giving you \\\n",
    "        an article and let me know if anything is potentially\\\n",
    "        false. Go with a fine tooth comb and have low sensitivity\\\n",
    "        for locating potential errors\"}, \n",
    "        {\"role\": \"user\", \"content\": wiki_content}]\n",
    "    )\n",
    "    return (chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPw5LyPEobmk"
   },
   "source": [
    "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "V7cuhML2ocGn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The information provided in the article appears to be largely accurate with comprehensive details on the field of artificial intelligence. However, there are some minor inaccuracies or areas that could be clarified:\n",
      "\n",
      "1. The article mentions that AI was founded as an academic discipline in 1956, which refers to the Dartmouth Conference that is often regarded as the birth of artificial intelligence as a field. However, it would be more accurate to mention that the term \"artificial intelligence\" was coined during this conference, formalizing the study of intelligent machines.\n",
      "\n",
      "2. The statement \"GPS\" is used in the context of generative and creative tools, mentioning \"ChatGPT and AI art.\" The correct reference should be \"GPT\" for Generative Pre-trained Transformer models, not \"GPS.\"\n",
      "\n",
      "3. There is a reference to the \"AI spring of the early 2020s,\" which implies a specific time frame. While AI advancements have indeed been accelerating in recent years, it might be more accurate to state that there has been continued progress in the field, especially with the advancements in deep learning and transformative architecture.\n",
      "\n",
      "4. In the section on knowledge representation, the article mentions a knowledge base and an ontology. It could be clarified that an ontology is a specific type of knowledge representation that includes objects, relations, concepts, and properties within a particular domain.\n",
      "\n",
      "5. In the section on planning and decision-making, a Markov decision process is described. It would be helpful to clarify that a Markov decision process is a mathematical framework for modeling decision-making in situations where outcomes are partially random and partially under the control of a decision-maker.\n",
      "\n",
      "Overall, the article provides a detailed overview of artificial intelligence, its applications, research goals, and sub-fields. Just a few minor corrections and clarifications would enhance the accuracy and understanding of the information presented.\n"
     ]
    }
   ],
   "source": [
    "page_titles = ['Artificial Intelligence','Machine Learning','UCLA']\n",
    "page_title = page_titles[0]\n",
    "text = get_wikipedia_content(page_title)\n",
    "print(chatgpt_error_correction(text[:8192]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size = 8180):\n",
    "    chunks = len(text)//8180 + 1\n",
    "    return([text[i*chunk_size:(i+1)*chunk_size]\\\n",
    "            for i in range(0,chunks-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8180"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_text(text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________Artificial Intelligence\n",
      "The article on artificial intelligence (AI) is detailed and thorough. However, there are a few potentially inaccurate statements or points that could be clarified:\n",
      "\n",
      "1. While Alan Turing did make significant contributions to the field of artificial intelligence with his work on machine intelligence, he was not the first person to conduct substantial research in this area. The concept of artificial intelligence predates Turing and can be traced back to the early 20th century with figures such as Ada Lovelace and John McCarthy playing key roles.\n",
      "\n",
      "2. The article suggests that artificial intelligence was founded as an academic discipline in 1956. While the Dartmouth Conference in 1956 is often cited as a key event that marked the beginning of AI as a field of study, it is important to note that research and work on AI-related concepts had been ongoing before this conference.\n",
      "\n",
      "3. The article mentions funding and interest significantly increasing after 2012 when deep learning surpassed all previous AI techniques. While deep learning has indeed played a crucial role in the advancements in AI, the field itself has seen continuous growth and development over the years with contributions from various subfields and techniques.\n",
      "\n",
      "4. The claim that the AI spring of the early 2020s saw significant advances predominantly from companies, universities, and laboratories based in the United States may be oversimplified. While the U.S. has been a prominent player in AI research and development, there are also notable contributions and advancements coming from other countries and regions around the world.\n",
      "\n",
      "Overall, the article provides a comprehensive overview of artificial intelligence and its various subfields, but some minor inaccuracies and nuances could be addressed for a more precise representation.\n",
      "________Machine Learning\n",
      "ERROR\n",
      "________UCLA\n",
      "I have identified some potentially false information in the article about UCLA:\n",
      "\n",
      "1. The statement that the academic roots of UCLA were established in 1881 as the southern branch of the California State Normal School, which later evolved into San Jose State University, is incorrect. UCLA was not the southern branch of what later became San Jose State University. Instead, UCLA was established as the southern branch of the California State Normal School in 1919.\n",
      "\n",
      "2. The article mentions that UCLA's athletic teams entered the Pacific Coast Conference in 1926. This information is inaccurate. UCLA did not join the Pacific Coast Conference (now known as the Pac-12 Conference) until 1959.\n",
      "\n",
      "Please let me know if you need clarification on any other points or if you would like me to look for more potential errors.\n"
     ]
    }
   ],
   "source": [
    "for page_title in page_titles:\n",
    "    print('________'+page_title)\n",
    "    try:\n",
    "        text = get_wikipedia_content(page_title)\n",
    "        print(chatgpt_error_correction(text[:8192]))\n",
    "    except:\n",
    "        print('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
